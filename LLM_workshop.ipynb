{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UG-Team-Data-Science/llm-workshop/blob/main/LLM_workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nD_Tl2Gh4H2N"
      },
      "source": [
        "# **Open-Source Large Language Models for Structured Information Extraction**\n",
        "\n",
        "Open-source large language models can be used to extract structured infomation from unstructured text. This notebook demonstrates doing so \"locally\" with the `llama.cpp` library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAkho8sZwWyr",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Connect to Google Drive\n",
        "\n",
        "# @markdown If you wish to connect to Google Drive, e.g. to load your own data from a folder, check this and  follow the instructions of the pop-up.\n",
        "\n",
        "connect_to_google_drive = False #@param {\"type\": \"boolean\"}\n",
        "\n",
        "import os\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  if connect_to_google_drive:\n",
        "    drive.mount(\"/content/gdrive\")\n",
        "  working_dir = Path(\"/content\")\n",
        "except ImportError:\n",
        "  # special case for Michiel\n",
        "  if os.path.exists(\"/nvme/storage_michiel/llm_workshop\"):\n",
        "    working_dir = Path(\"/nvme/storage_michiel/llm_workshop\")\n",
        "  else:\n",
        "    warnings.warn(\"You're not running this on Google Colab, confirgure the working directory (`working_dir`) to something sensical for your machine\")\n",
        "    working_dir = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTQJPc600ybq",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Install `llama-cpp` and download model\n",
        "%%capture\n",
        "\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# install llama_cpp if not already\n",
        "!python3 -c 'from llama_cpp import Llama' 2> /dev/null || (CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python'==0.2.53')\n",
        "from llama_cpp import Llama\n",
        "\n",
        "use_model = \"openhermes-2.5-mistral-7b.Q5_K_M\"\n",
        "repo_id = \"TheBloke/OpenHermes-2.5-Mistral-7B-GGUF\"\n",
        "\n",
        "model_filename = hf_hub_download(\n",
        "    repo_id=repo_id,\n",
        "    filename=use_model + \".gguf\",\n",
        "    repo_type=\"model\",\n",
        "    local_dir=working_dir,\n",
        "    token=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FBJKf0IZZst",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Instantiate the local LLM\n",
        "%%capture\n",
        "\n",
        "llm = Llama(\n",
        "    model_path=model_filename,\n",
        "    n_gpu_layers=-1,\n",
        "    n_ctx=8000,\n",
        "    random_seed=42,\n",
        ")\n",
        "llm.verbose=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JH6KrBBKKycv",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Define helper functions\n",
        "from pprint import pprint, pp, pformat\n",
        "\n",
        "template = \"\"\"<|im_start|>system\n",
        "You are a helpful assistant.<|im_end|>\n",
        "<|im_start|>user\n",
        "{prompt}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "\n",
        "def local_llm(prompt, verbose=False, apply_template=True, temperature=0.7, max_tokens=None):\n",
        "    if apply_template:\n",
        "        prompt = template.format(prompt=prompt)\n",
        "    if verbose:\n",
        "        print(f\"Prompt:\\n{prompt}\")\n",
        "    response = llm(prompt, max_tokens=max_tokens, temperature=temperature, top_p=0.95)\n",
        "    return response[\"choices\"][0][\"text\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZL1g-7mXJ0h"
      },
      "source": [
        "# Prompting basics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QABtlt8OI0pd"
      },
      "outputs": [],
      "source": [
        "response = local_llm(\n",
        "    \"Write promotional material for a workshop demonstrating use cases of open-source large language models\"\n",
        ")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQ5tiiM3vkR9"
      },
      "source": [
        "## Chat templates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGGbrjdnK-Eg"
      },
      "outputs": [],
      "source": [
        "response = local_llm(\n",
        "    \"In what city is Campus Fryslan located?\",\n",
        "    verbose=True,\n",
        ")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUaokB2AKSNt"
      },
      "outputs": [],
      "source": [
        "response = local_llm(\n",
        "    \"In what city is Campus Fryslan located?\",\n",
        "    apply_template=False,\n",
        "    verbose=True,\n",
        "    temperature=0.0\n",
        ")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_prompt = \"\"\"<|im_start|>system\n",
        "Your are a helpful assistant that answers in the style of a pirate.<|im_end|>\n",
        "<|im_start|>user\n",
        "In what city is Campus Fryslan located?<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "response = local_llm(\n",
        "    formatted_prompt,\n",
        "    apply_template=False,\n",
        "    verbose=True,\n",
        "    temperature=0.0\n",
        ")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "MW4FiZjQHMxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk3A5Z31NYmt"
      },
      "source": [
        "## Temperature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70Ml_TVONX_X"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "I'm organizing a workshop on using LLMs to extract structured information from texts / corpora for non-technical researchers at a university.\n",
        "Suggest a few catchy titles, free of jargon.\n",
        "\"\"\"\n",
        "\n",
        "response = local_llm(prompt, temperature=0.0)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rA8PW7E_KLP0"
      },
      "outputs": [],
      "source": [
        "response = local_llm(prompt, temperature=0.0)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-lQJeUoHie3"
      },
      "outputs": [],
      "source": [
        "response = local_llm(prompt, temperature=0.9)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTQc29RmOmje"
      },
      "outputs": [],
      "source": [
        "response = local_llm(prompt, temperature=0.9)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUorb-qjzVcw"
      },
      "source": [
        "## Number of input / output tokens\n",
        "\n",
        "- What is a token?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1k46xd5OtyW"
      },
      "outputs": [],
      "source": [
        "response = local_llm(prompt, max_tokens=20)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% capture\n",
        "!wget https://www.gutenberg.org/cache/epub/100/pg100.txt -P $working_dir\n",
        "long_text = (working_dir / \"pg100.txt\").read_text(encoding=\"utf-8\")\n"
      ],
      "metadata": {
        "id": "0mDVKIGo9k7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "long_prompt = \"Please summarize the following: \\n\" + long_text\n",
        "# response = local_llm(long_prompt)"
      ],
      "metadata": {
        "id": "cmbBXwOA_WEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a34gj5t1XjBX"
      },
      "source": [
        "# Prompt Engineering 101\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPQ5XuPsWzAU"
      },
      "outputs": [],
      "source": [
        "# @title Zero-shot prompting\n",
        "prompt = \"\"\"\n",
        "Review: I think the workshop was okay.\n",
        "Sentiment: ?\n",
        "\"\"\"\n",
        "print(local_llm(prompt))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Few-shot prompting\n",
        "prompt = \"\"\"\n",
        "Review: The workshop was enlightening! Engaging speaker, loads of insights. Excited to apply learnings!\n",
        "Sentiment: Positive\n",
        "\n",
        "Review: LLM workshop disappointed. Speaker unprepared, content basic. Would not recommended\n",
        "Sentiment: Negative\n",
        "\n",
        "Review: LLM workshop was fantastic! Expert speaker, hands-on activities. Left feeling inspired!\n",
        "Sentiment: ?\n",
        "\"\"\"\n",
        "local_llm(prompt)"
      ],
      "metadata": {
        "id": "whPnljsSQjxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Chain-of-thought prompting\n",
        "\n",
        "prompt = \"\"\"I need 10 eggs to make a cake.\n",
        "I have one egg in my fridge.\n",
        "I went to the market and bought two cartons with four eggs each.\n",
        "\n",
        "Do I have enough eggs now?\n",
        "\"\"\"\n",
        "print(local_llm(prompt, temperature=0.0))"
      ],
      "metadata": {
        "id": "dJ72m_2pDA4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt =  \"\"\"\n",
        "I need 10 eggs to make a cake.\n",
        "I have one egg in my fridge.\n",
        "I went to the market and bought two cartons with four eggs each.\n",
        "Do I have enough eggs now?\n",
        "\n",
        "Think step by step.\n",
        "\"\"\"\n",
        "print(local_llm(prompt, temperature=0.0))"
      ],
      "metadata": {
        "id": "uOliGtB7EjVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt =  \"\"\"\n",
        "I need 10 eggs to make a cake.\n",
        "I have one egg in my fridge.\n",
        "I went to the market and bought two cartons with four eggs each.\n",
        "\n",
        "Do I have enough eggs now?\n",
        "Think step by step.\n",
        "Explain each intermediate step.\n",
        "Only when you are done with all your steps,\n",
        "provide the answer based on your intermediate steps.\n",
        "\"\"\"\n",
        "print(local_llm(prompt, temperature=0.0))"
      ],
      "metadata": {
        "id": "b6jGXi6YZ8q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "I need 10 eggs to make a cake.\n",
        "I have one egg in my fridge.\n",
        "I went to the market and bought two cartons with four eggs each.\n",
        "\n",
        "Do I have enough eggs now?\n",
        "Think step by step.\n",
        "Provide the answer as a single yes/no answer first.\n",
        "Then explain each intermediate step.\n",
        "\"\"\"\n",
        "print(local_llm(prompt, temperature=0.0))"
      ],
      "metadata": {
        "id": "TiUT7qVzbB9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now try to experiment with your own prompt! Note that smaller language models work best on \"interpolation\" - analyzing within the context rather than generating completely new text.\n",
        "\n",
        "Some tasks to consider\n",
        "- Classify a piece of text, supplying a list of possible labels.\n",
        "- Extracting information from a piece of text, e.g. certain characterics associated with a person in the text\n",
        "- Summarizing\n",
        "\n",
        "Some tips:\n",
        "- Use delimiters to separate parts of your input\n",
        "- Give specific instructions\n",
        "- Rerun the prompt a few times to get an idea of the variance of the responses\n",
        "- Investigate the effect of encouraging chain-of-thought\n",
        "\n",
        "\n",
        "If you're unable to get good results it might very well be due to the limitations of the model we're using here! As a sanity check, you can run your prompt on more powerful models here: https://chat.lmsys.org/?single&model=llama-2-70b-chat (this link loads a 70B Llama 2 model by default, but commercial, closed source models are available as well)"
      ],
      "metadata": {
        "id": "l031yYZWz-sZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "\"\"\"\n",
        "print(local_llm(prompt))"
      ],
      "metadata": {
        "id": "y3z9oeGVz60v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Scaling up\n",
        "\n",
        "- Prompt template\n",
        "- Structure output\n",
        "- Retry until structure is valid\n"
      ],
      "metadata": {
        "id": "8xpIgOIbcT3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Fetch Data and Load Into Pandas\n",
        "# @markdown Based on *Powell-Smith A., Centre for Public Data, Analysis of Unanswered Questions in the UK Parliament (2022), GitHub repository, https://github.com/centreforpublicdata/written-answers.*\n",
        "\n",
        "%%capture\n",
        "\n",
        "!wget \"http://datascience.web.rug.nl/llm_parliamentary_sample.csv\" -P $working_dir\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv(working_dir / \"llm_parliamentary_sample.csv\")"
      ],
      "metadata": {
        "id": "cRs-LOUwcVZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.query(\"votes_diff > 0\").head()"
      ],
      "metadata": {
        "id": "hK5O6ZujcsME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_row = df.query(\"votes_diff > 0\").iloc[0]"
      ],
      "metadata": {
        "id": "siUK4s6ed4uT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt templates, structuring outputs\n",
        "\n",
        "- Obtain machine-parseable outputs by requesting a JSON object"
      ],
      "metadata": {
        "id": "J8NVlCIXEKq6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31eeyzdmxo0K"
      },
      "outputs": [],
      "source": [
        "\n",
        "prompt_template = \"\"\"\n",
        "I will provide you a question and a response given in a parliamentary setting.\n",
        "\n",
        "The question:\n",
        "*********\n",
        "{question}\n",
        "*********\n",
        "\n",
        "The answer:\n",
        "*********\n",
        "{answer}\n",
        "*********\n",
        "\n",
        "Does the response sufficiently answer the question?\n",
        "\n",
        "Return your answer as a valid JSON object with a single field `final answer` with\n",
        "a boolean value with your final answer, like {{\"final_answer\": …}}.\n",
        "\"\"\"\n",
        "\n",
        "prompt = prompt_template.format(\n",
        "    question=first_row[\"question_text\"].strip(),\n",
        "    answer=first_row[\"answer_text\"].strip()\n",
        ")\n",
        "\n",
        "response = local_llm(prompt)\n",
        "print(\"\\nLLM answer: \")\n",
        "print(response)\n",
        "\n",
        "response = local_llm(prompt + \"\\n\\Think step by step\")\n",
        "print(\"\\nLLM answer (CoT): \")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Sometimes, chain-of-thought can be prompted more effectively by \"jumpstarting\" the agent's reponse"
      ],
      "metadata": {
        "id": "JKlnIeYCSNEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_prompt_template = \"\"\"<|im_start|>system\n",
        "You are a helpful assistant.<|im_end|>\n",
        "<|im_start|>user\n",
        "\n",
        "I will provide you a question and a response given in a parliamentary setting.\n",
        "\n",
        "The question:\n",
        "```\n",
        "{question}\n",
        "```\n",
        "\n",
        "The answer:\n",
        "```\n",
        "{answer}\n",
        "```\n",
        "\n",
        "Does the response sufficiently answer the question?\n",
        "\n",
        "Return your answer as a valid JSON object with a single field `final answer` with\n",
        "a boolean value with your final answer, like {{\"final_answer\": …}}.\n",
        "<|im_end|>\n",
        "<|im_start|>assistant\n",
        "Let's think step by step:\n",
        "\"\"\"\n",
        "\n",
        "formatted_prompt = formatted_prompt_template.format(\n",
        "    question=first_row[\"question_text\"].strip(),\n",
        "    answer=first_row[\"answer_text\"].strip()\n",
        ")\n",
        "print(formatted_prompt)"
      ],
      "metadata": {
        "id": "VjwRtmbAB89s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = local_llm(formatted_prompt, apply_template=False)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "QCVEN2-9Ce3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parsing the answer from the response"
      ],
      "metadata": {
        "id": "a-OcSZUzDVLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define helper functions\n",
        "\n",
        "\n",
        "import re\n",
        "import json\n",
        "from json import JSONDecodeError\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "json_expression = re.compile(r\"\\{.+?\\}\", re.DOTALL)\n",
        "\n",
        "\n",
        "def can_parse(model_output, output_arguments, output_types=None):\n",
        "    if output_types is None:\n",
        "        output_types = dict()\n",
        "    answers = json_expression.findall(model_output)\n",
        "    if len(answers) != 1:\n",
        "        return False\n",
        "    answer = answers[0]\n",
        "    try:\n",
        "        output = json.loads(answer)\n",
        "        for arg in output_arguments:\n",
        "            value = output[arg]\n",
        "            if arg in output_types:\n",
        "                if not isinstance(value, output_types[arg]):\n",
        "                    return False\n",
        "        return True\n",
        "    except (JSONDecodeError, KeyError):\n",
        "        return False\n",
        "\n",
        "def parse_output(model_output):\n",
        "    answers = json_expression.findall(model_output)\n",
        "    answer = answers[0]\n",
        "    return json.loads(answer)\n",
        "\n",
        "\n",
        "def annotation_loop(\n",
        "    input_df, apply_template, expected_keys, expected_types=None, n_retries=10\n",
        "):\n",
        "    df = input_df.copy()\n",
        "    df[\"can_parse\"] = False\n",
        "    for _ in range(n_retries):\n",
        "        not_parseable = ~df[\"can_parse\"]\n",
        "        responses = [\n",
        "            local_llm(prompt, apply_template=apply_template)\n",
        "            for prompt in tqdm(df.loc[not_parseable, \"prompt\"])\n",
        "        ]\n",
        "        df.loc[not_parseable, \"response\"] = responses\n",
        "        df.loc[not_parseable, \"can_parse\"] = df.loc[not_parseable, \"response\"].apply(\n",
        "            can_parse, args=(expected_keys, expected_types)\n",
        "        )\n",
        "        if df[\"can_parse\"].all():\n",
        "            break\n",
        "    parseable = df[\"can_parse\"]\n",
        "    df.loc[parseable, \"json\"] = df.loc[parseable, \"response\"].apply(parse_output)\n",
        "    for key in expected_keys:\n",
        "        df.loc[parseable, key] = df.loc[parseable, \"json\"].apply(lambda x: x[key])\n",
        "    return df.drop(\"json\", axis=\"columns\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jzJOzSe75lUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sampled = pd.concat(\n",
        "    (\n",
        "        df.sort_values(\"votes_diff\").iloc[:5],\n",
        "        df.sort_values(\"votes_diff\", ascending=False).iloc[:5],\n",
        "    )\n",
        ").copy()\n",
        "\n",
        "\n",
        "n_retries = 10\n",
        "\n",
        "expected_keys = [\"final_answer\"]\n",
        "expected_types = {\"final_answer\": bool}\n",
        "\n",
        "for idx, row in df_sampled.iterrows():\n",
        "    df_sampled.loc[idx, \"prompt\"] = (\n",
        "        formatted_prompt_template.format(\n",
        "            question=row.question_text.strip(), answer=row.answer_text.strip()\n",
        "        )\n",
        "    )\n",
        "\n",
        "print(df_sampled[\"prompt\"].iloc[0])"
      ],
      "metadata": {
        "id": "9phuM7jDYHfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_annotated = annotation_loop(df_sampled, apply_template=False, expected_keys=expected_keys, expected_types=expected_types)"
      ],
      "metadata": {
        "id": "NDxURBzUBeUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_annotated[[\"final_answer\", \"votes_diff\"]]"
      ],
      "metadata": {
        "id": "-ZsQlL1B7BeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_index = 533\n",
        "\n",
        "print(df_annotated.loc[selected_index,\"response\"], end=\"\\n\\n\")\n",
        "print(df_annotated.loc[selected_index, 'question_text'], end=\"\\n\\n\")\n",
        "print(df_annotated.loc[selected_index, 'answer_text'], end=\"\\n\\n\")\n"
      ],
      "metadata": {
        "id": "6P9eZjrS0RIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LavYhbD7R3mL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}