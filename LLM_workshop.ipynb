{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nD_Tl2Gh4H2N"
   },
   "source": [
    "# **Open-Source Large Language Models for Structured Information Extraction**\n",
    "\n",
    "Open-source large language models can be used to extract structured infomation from unstructured text. This notebook demonstrates doing so \"locally\" with the `llama.cpp` library\n",
    "\n",
    "\n",
    "Points for speaker:\n",
    "- Why are we using Colab?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mrzPdWQahBKe"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "working_dir = Path(\n",
    "    \"/nvme/storage_michiel/llm_workshop\"\n",
    ")  # /content when working with remote runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FAkho8sZwWyr"
   },
   "outputs": [],
   "source": [
    "# @title Connect to Google Drive\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/gdrive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "XTQJPc600ybq"
   },
   "outputs": [],
   "source": [
    "# @title Imports and downloads\n",
    "%%capture\n",
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python\n",
    "#!wget https://huggingface.co/NousResearch/Hermes-2-Pro-Mistral-7B-GGUF/resolve/main/Hermes-2-Pro-Mistral-7B.Q5_K_M.gguf -P $working_dir\n",
    "# !wget https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-GGUF/resolve/main/openhermes-2.5-mistral-7b.Q5_K_M.gguf -P $working_dir\n",
    "!wget https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-GGUF/resolve/main/openhermes-2.5-mistral-7b.Q4_K_M.gguf -P $working_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "2FBJKf0IZZst"
   },
   "outputs": [],
   "source": [
    "# @title Instantiate the local LLM\n",
    "%%capture\n",
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=str(working_dir / \"Hermes-2-Pro-Mistral-7B.Q5_K_M.gguf\"),\n",
    "    n_gpu_layers=-1,\n",
    "    n_ctx=8192,\n",
    "    random_seed=42,\n",
    ")\n",
    "llm.verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "JH6KrBBKKycv"
   },
   "outputs": [],
   "source": [
    "# @title Define helper functions\n",
    "from pprint import pformat, pp, pprint\n",
    "\n",
    "template = \"\"\"\n",
    "<|im_start|>user\n",
    "{prompt}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def local_llm(\n",
    "    prompt, verbose=False, apply_template=True, temperature=0.7, max_tokens=None\n",
    "):\n",
    "    if apply_template:\n",
    "        prompt = template.format(prompt=prompt)\n",
    "    if verbose:\n",
    "        print(f\"Prompt:\\n{prompt}\")\n",
    "    response = llm(prompt, max_tokens=max_tokens, temperature=temperature, top_p=0.95)\n",
    "    return response[\"choices\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCeopZooH_Fp"
   },
   "source": [
    "- Overview of different models, sizes\n",
    "- Foundation/base models vs chat / instruction models\n",
    "- \"Access / Privacy\"\n",
    "- `llama-cpp`!\n",
    "- quantization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZL1g-7mXJ0h"
   },
   "source": [
    "# Prompting basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QABtlt8OI0pd"
   },
   "outputs": [],
   "source": [
    "response = local_llm(\n",
    "    \"Write me promotional material for a workshop demonstrating use cases of open-source large language models\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JB6OKm_vLAcg"
   },
   "source": [
    "- Explain what happened - we called a local LLM!\n",
    "- Chat template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQ5tiiM3vkR9"
   },
   "source": [
    "## Chat templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wGGbrjdnK-Eg"
   },
   "outputs": [],
   "source": [
    "response = local_llm(\n",
    "    \"In what city is Campus Fryslan located?\",\n",
    "    verbose=True,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qUaokB2AKSNt"
   },
   "outputs": [],
   "source": [
    "response = local_llm(\n",
    "    \"In what city is Campus Fryslan located?\",\n",
    "    apply_template=False,\n",
    "    verbose=True,\n",
    "    temperature=0.0,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qk3A5Z31NYmt"
   },
   "source": [
    "## Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "70Ml_TVONX_X"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "I'm organizing a workshop on using LLMs to extract structured information from\n",
    "texts / corpora for non-technical researchers at a university.\n",
    "Could you suggest me a few catchy titles, free of jargon?\n",
    "\"\"\"\n",
    "\n",
    "response = local_llm(prompt, temperature=0.0)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rA8PW7E_KLP0"
   },
   "outputs": [],
   "source": [
    "response = local_llm(prompt, temperature=0.0)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O-lQJeUoHie3"
   },
   "outputs": [],
   "source": [
    "response = local_llm(prompt, temperature=0.9)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oTQc29RmOmje"
   },
   "outputs": [],
   "source": [
    "response = local_llm(prompt, temperature=0.9)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUorb-qjzVcw"
   },
   "source": [
    "## Number of input / output tokens\n",
    "\n",
    "- What is a token?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x1k46xd5OtyW"
   },
   "outputs": [],
   "source": [
    "response = local_llm(prompt, max_tokens=20)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0mDVKIGo9k7K"
   },
   "outputs": [],
   "source": [
    "!wget https://www.gutenberg.org/cache/epub/100/pg100.txt -P $working_dir\n",
    "long_text = (working_dir / \"pg100.txt\").read_text(encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o5lokUP-_kVM"
   },
   "outputs": [],
   "source": [
    "print(long_text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cmbBXwOA_WEw"
   },
   "outputs": [],
   "source": [
    "long_prompt = \"Please summarize the following: \\n\" + long_text\n",
    "response = local_llm(long_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a34gj5t1XjBX"
   },
   "source": [
    "# Prompt Engineering 101\n",
    "\n",
    "- Zero shot learning\n",
    "- Few shot learning\n",
    "- Chain of thought\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MPQ5XuPsWzAU"
   },
   "outputs": [],
   "source": [
    "# @title Zero-shot prompting\n",
    "prompt = \"\"\"\n",
    "Classify the text into neutral, negative or positive.\n",
    "Text: I think the workshop is okay.\n",
    "\"\"\"\n",
    "print(local_llm(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "whPnljsSQjxd"
   },
   "outputs": [],
   "source": [
    "# @title One-shot prompting\n",
    "prompt = \"\"\"\n",
    "A \"whatpu\" is a small, furry animal native to Tanzania. An example of a sentence that uses the word whatpu is:\n",
    "We were traveling in Africa and we saw these very cute whatpus.\n",
    "\n",
    "To do a \"farduddle\" means to jump up and down really fast. Please give an example of a sentence that uses the word farduddle.\n",
    "\"\"\"\n",
    "local_llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dJ72m_2pDA4S"
   },
   "outputs": [],
   "source": [
    "# @title Chain-of-thought prompting\n",
    "\n",
    "prompt_no_cot_formatted = \"\"\"\n",
    "<|im_start|>user\n",
    "Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls.\n",
    "How many tennis balls does he have now?\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "The answer is 11.\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "The cafetaria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "print(local_llm(prompt_no_cot_formatted, apply_template=False, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uOliGtB7EjVD"
   },
   "outputs": [],
   "source": [
    "prompt_cot_formatted = \"\"\"\n",
    "<|im_start|>user\n",
    "Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls.\n",
    "How many tennis balls does he have now?\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "The cafetaria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "print(local_llm(prompt_cot_formatted, apply_template=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9mDjr6EeTwiQ"
   },
   "outputs": [],
   "source": [
    "# @title Zero-shot chain-of-thought\n",
    "\n",
    "prompt_cot_zs = \"\"\"\n",
    "<|im_start|>user\n",
    "The cafetaria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "Let's think step by step: \"\"\"\n",
    "print(local_llm(prompt_cot_zs, apply_template=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xpIgOIbcT3l"
   },
   "source": [
    "\n",
    "# Scaling up\n",
    "\n",
    "- Prompt template\n",
    "- Structure output\n",
    "- Retry until structure is valid\n",
    "- External APIs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "cRs-LOUwcVZa"
   },
   "outputs": [],
   "source": [
    "# @title Fetch Data and Load Into Pandas\n",
    "%%capture\n",
    "\n",
    "!wget \"http://datascience.web.rug.nl/llm_parliamentary_sample.csv\" -P $working_dir\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(working_dir / \"llm_parliamentary_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hK5O6ZujcsME"
   },
   "outputs": [],
   "source": [
    "df.query(\"votes_diff > 0\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "siUK4s6ed4uT"
   },
   "outputs": [],
   "source": [
    "first_row = df.query(\"votes_diff > 0\").iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "31eeyzdmxo0K"
   },
   "outputs": [],
   "source": [
    "# @title Prompt templates, structuring outputs\n",
    "\n",
    "formatted_prompt_template = \"\"\"\n",
    "<|im_start|>user\n",
    "I will provide you a question and a response given in a parliamentary setting.\n",
    "\n",
    "The question:\n",
    "*********\n",
    "{question}\n",
    "*********\n",
    "\n",
    "The answer:\n",
    "*********\n",
    "{answer}\n",
    "*********\n",
    "\n",
    "Does the response sufficiently answer the question?\n",
    "\n",
    "Return your answer as a valid JSON object with a single field `final answer` with\n",
    "a boolean value with your final answer, like {{\"final_answer\": â€¦}}.\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "formatted_prompt = formatted_prompt_template.format(\n",
    "    question=first_row[\"question_text\"].strip(), answer=first_row[\"answer_text\"].strip()\n",
    ")\n",
    "\n",
    "response = local_llm(formatted_prompt, apply_template=False, verbose=True)\n",
    "print(\"\\nLLM answer: \")\n",
    "print(response)\n",
    "\n",
    "response = local_llm(\n",
    "    formatted_prompt + \"Let's think step by step: \", apply_template=False, verbose=True\n",
    ")\n",
    "print(\"\\nLLM answer (Zero-shot CoT): \")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-OcSZUzDVLE"
   },
   "source": [
    "# Parsing the answer from the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "jzJOzSe75lUm"
   },
   "outputs": [],
   "source": [
    "# @title Define helper functions\n",
    "\n",
    "\n",
    "import json\n",
    "import re\n",
    "from json import JSONDecodeError\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "json_expression = re.compile(r\"\\{.+?\\}\", re.DOTALL)\n",
    "\n",
    "\n",
    "def can_parse(model_output, output_arguments, output_types=None):\n",
    "    if output_types is None:\n",
    "        output_types = dict()\n",
    "    answers = json_expression.findall(model_output)\n",
    "    if len(answers) != 1:\n",
    "        return False\n",
    "    answer = answers[0]\n",
    "    try:\n",
    "        output = json.loads(answer)\n",
    "        for arg in output_arguments:\n",
    "            value = output[arg]\n",
    "            if arg in output_types:\n",
    "                if not isinstance(value, output_types[arg]):\n",
    "                    return False\n",
    "        return True\n",
    "    except (JSONDecodeError, KeyError):\n",
    "        return False\n",
    "\n",
    "\n",
    "def parse_output(model_output):\n",
    "    answers = json_expression.findall(model_output)\n",
    "    answer = answers[0]\n",
    "    return json.loads(answer)\n",
    "\n",
    "\n",
    "def annotation_loop(\n",
    "    input_df, apply_template, expected_keys, expected_types=None, n_retries=10\n",
    "):\n",
    "    df = input_df.copy()\n",
    "    df[\"can_parse\"] = False\n",
    "    for _ in range(n_retries):\n",
    "        not_parseable = ~df[\"can_parse\"]\n",
    "        responses = [\n",
    "            local_llm(prompt, apply_template=apply_template)\n",
    "            for prompt in tqdm(df.loc[not_parseable, \"formatted_prompt\"])\n",
    "        ]\n",
    "        df.loc[not_parseable, \"response\"] = responses\n",
    "        df.loc[not_parseable, \"can_parse\"] = df.loc[not_parseable, \"response\"].apply(\n",
    "            can_parse, args=(expected_keys, expected_types)\n",
    "        )\n",
    "        if df[\"can_parse\"].all():\n",
    "            break\n",
    "    parseable = df[\"can_parse\"]\n",
    "    df.loc[parseable, \"json\"] = df.loc[parseable, \"response\"].apply(parse_output)\n",
    "    for key in expected_keys:\n",
    "        df.loc[parseable, key] = df.loc[parseable, \"json\"].apply(lambda x: x[key])\n",
    "    return df.drop(\"json\", axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9phuM7jDYHfd"
   },
   "outputs": [],
   "source": [
    "df_sampled = pd.concat(\n",
    "    (\n",
    "        df.sort_values(\"votes_diff\").iloc[:5],\n",
    "        df.sort_values(\"votes_diff\", ascending=False).iloc[:5],\n",
    "    )\n",
    ").copy()\n",
    "\n",
    "\n",
    "n_retries = 10\n",
    "\n",
    "expected_keys = [\"final_answer\"]\n",
    "expected_types = {\"final_answer\": bool}\n",
    "\n",
    "for idx, row in df_sampled.iterrows():\n",
    "    df_sampled.loc[idx, \"formatted_prompt\"] = (\n",
    "        formatted_prompt_template.format(\n",
    "            question=row.question_text.strip(), answer=row.answer_text.strip()\n",
    "        )\n",
    "        + \"Let's think step by step: \"\n",
    "    )\n",
    "\n",
    "print(df_sampled[\"formatted_prompt\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NDxURBzUBeUI"
   },
   "outputs": [],
   "source": [
    "df_annotated = annotation_loop(\n",
    "    df_sampled,\n",
    "    apply_template=False,\n",
    "    expected_keys=expected_keys,\n",
    "    expected_types=expected_types,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ZsQlL1B7BeQ"
   },
   "outputs": [],
   "source": [
    "df_annotated[[\"final_answer\", \"votes_diff\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LavYhbD7R3mL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
