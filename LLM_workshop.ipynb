{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nD_Tl2Gh4H2N"
      },
      "source": [
        "# **Open-Source Large Language Models for Structured Information Extraction**\n",
        "\n",
        "Open-source large language models can be used to extract structured infomation from unstructured text. This notebook demonstrates doing so \"locally\" with the `llama.cpp` library\n",
        "\n",
        "\n",
        "Points for speaker:\n",
        "- Why are we using Colab?\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "working_dir = Path(\"/nvme/storage_michiel/llm_workshop\") #/content when working with remote runtime"
      ],
      "metadata": {
        "id": "mrzPdWQahBKe"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAkho8sZwWyr",
        "outputId": "c65bfdf6-2557-4fa4-ff8a-3ac9bd289db8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# @title Connect to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "XTQJPc600ybq"
      },
      "outputs": [],
      "source": [
        "# @title Imports and downloads\n",
        "%%capture\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python\n",
        "#!wget https://huggingface.co/NousResearch/Hermes-2-Pro-Mistral-7B-GGUF/resolve/main/Hermes-2-Pro-Mistral-7B.Q5_K_M.gguf -P $working_dir\n",
        "!wget https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-GGUF/resolve/main/openhermes-2.5-mistral-7b.Q5_K_M.gguf -P $working_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RAjEYNJA75jk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "2FBJKf0IZZst"
      },
      "outputs": [],
      "source": [
        "# @title Instantiate the local LLM\n",
        "%%capture\n",
        "from llama_cpp import Llama\n",
        "\n",
        "llm = Llama(\n",
        "    model_path=str(working_dir / \"openhermes-2.5-mistral-7b.Q5_K_M.gguf\"),\n",
        "    n_gpu_layers=-1,\n",
        "    n_ctx=8192,\n",
        "    random_seed=42,\n",
        ")\n",
        "llm.verbose=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "JH6KrBBKKycv",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Define helper functions\n",
        "from pprint import pprint, pp, pformat\n",
        "\n",
        "template = \"\"\"\n",
        "<|im_start|>user\n",
        "{prompt}\n",
        "<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "\n",
        "def local_llm(prompt, verbose=False, apply_template=True, temperature=0.7, max_tokens=None):\n",
        "    if apply_template:\n",
        "        prompt = template.format(prompt=prompt)\n",
        "    if verbose:\n",
        "        print(f\"Prompt:\\n{prompt}\")\n",
        "    response = llm(prompt, max_tokens=max_tokens, temperature=temperature, top_p=0.95)\n",
        "    return response[\"choices\"][0][\"text\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCeopZooH_Fp"
      },
      "source": [
        "- Overview of different models, sizes\n",
        "- Foundation/base models vs chat / instruction models\n",
        "- \"Access / Privacy\"\n",
        "- `llama-cpp`!\n",
        "- quantization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZL1g-7mXJ0h"
      },
      "source": [
        "# Prompting basics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QABtlt8OI0pd",
        "outputId": "1e6c3bbf-f558-443b-86be-4060d3327e89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Introducing: \"Unleashing the Power of Open-Source Large Language Models\" Workshop\n",
            "\n",
            "Are you ready to revolutionize your approach to natural language processing and generation? Join us for our upcoming workshop, where we will showcase the incredible power and potential of open-source large language models.\n",
            "\n",
            "In this interactive and engaging session, you will discover how these cutting-edge technologies are transforming industries and enhancing communication. Our expert trainers will guide you through real-world use cases, providing practical insights and hands-on experience.\n",
            "\n",
            "Key Takeaways:\n",
            "\n",
            "1. Understanding the fundamentals of open-source large language models and their applications.\n",
            "2. Exploring various open-source options, such as Hugging Face's Transformers and TensorFlow.\n",
            "3. Identifying and analyzing potential use cases in various sectors, including healthcare, finance, and education.\n",
            "4. Learning best practices and techniques for implementing these models in your projects.\n",
            "5. Networking with like-minded professionals and staying up-to-date with the latest trends and developments in natural language processing.\n",
            "\n",
            "Don't miss this opportunity to expand your knowledge and skillset in the rapidly growing field of open-source large language models. Register now and join us for an unforgettable experience!\n"
          ]
        }
      ],
      "source": [
        "response = local_llm(\n",
        "    \"Write me promotional material for a workshop demonstrating use cases of open-source large language models\"\n",
        ")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JB6OKm_vLAcg"
      },
      "source": [
        "- Explain what happened - we called a local LLM!\n",
        "- Chat template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQ5tiiM3vkR9"
      },
      "source": [
        "## Chat templates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGGbrjdnK-Eg",
        "outputId": "2b87af01-20b4-4576-a9b2-5f720fbc9d19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt:\n",
            "\n",
            "<|im_start|>user\n",
            "In what city is Campus Fryslan located?\n",
            "<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Campus Fryslan is located in Leeuwarden, Netherlands. It is an educational institution that focuses on providing international students with the opportunity to learn Dutch as a foreign language while experiencing the local culture and history of the region.\n"
          ]
        }
      ],
      "source": [
        "response = local_llm(\n",
        "    \"In what city is Campus Fryslan located?\",\n",
        "    verbose=True,\n",
        ")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUaokB2AKSNt",
        "outputId": "2630ee9b-e62b-442e-c218-413db59dee00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt:\n",
            "In what city is Campus Fryslan located?\n",
            "\n",
            "Campus Fryslan is located in Leeuwarden, the capital of Friesland.\n",
            "\n",
            "What are the admission requirements for Campus Fryslan?\n",
            "To be admitted to Campus Fryslan, you must have a secondary school diploma or equivalent and meet the specific entry requirements for your chosen program. Some programs may also require additional tests or interviews.\n",
            "\n",
            "How many students attend Campus Fryslan?\n",
            "Campus Fryslan has approximately 1,500 students enrolled in its various programs.\n",
            "\n",
            "What programs does Campus Fryslan offer?\n",
            "Campus Fryslan offers a range of programs in fields such as business, tourism, hospitality, and leisure management. These programs are designed to provide students with practical skills and knowledge that can be applied in a variety of industries.\n",
            "\n",
            "Does Campus Fryslan offer international programs?\n",
            "Yes, Campus Fryslan offers international programs that are taught in English and designed for students from around the world. These programs provide an opportunity for students to gain a global perspective and develop their skills in an international context.\n",
            "\n",
            "Does Campus Fryslan offer scholarships or financial aid?\n",
            "Yes, Campus Fryslan offers scholarships and financial aid to eligible students. These programs are designed to help students cover the costs of tuition and other expenses associated with attending college.\n",
            "\n",
            "What facilities does Campus Fryslan have?\n",
            "Campus Fryslan has a range of facilities to support students' learning and development. These include classrooms, computer labs, a library, a cafeteria, and sports facilities. The campus also has a student center that provides a space for students to socialize and participate in extracurricular activities.\n"
          ]
        }
      ],
      "source": [
        "response = local_llm(\n",
        "    \"In what city is Campus Fryslan located?\",\n",
        "    apply_template=False,\n",
        "    verbose=True,\n",
        "    temperature=0.0\n",
        ")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk3A5Z31NYmt"
      },
      "source": [
        "## Temperature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70Ml_TVONX_X",
        "outputId": "3ece5264-f468-43dd-a763-9c4e28c5cf94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. \"Unlocking the Power of AI: Extracting Structured Information from Texts\"\n",
            "2. \"Transforming Text into Data: A Non-Technical Guide to AI Extraction\"\n",
            "3. \"Revolutionize Your Research: Harnessing AI to Extract Key Information\"\n",
            "4. \"From Text to Insights: A Workshop on AI-Driven Information Extraction\"\n",
            "5. \"AI for Everyone: Simplifying the Process of Extracting Structured Data\"\n",
            "6. \"Beyond Keywords: AI Techniques for Extracting Meaningful Information\"\n",
            "7. \"AI-Assisted Research: Extracting Structured Data from Unstructured Text\"\n",
            "8. \"AI Meets Research: A Hands-On Workshop on Extracting Structured Information\"\n",
            "9. \"AI-Powered Text Analysis: Extracting Valuable Data for Your Research\"\n",
            "10. \"AI and You: A Beginner's Guide to Extracting Structured Information from Texts\"\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "I'm organizing a workshop on using LLMs to extract structured information from\n",
        "texts / corpora for non-technical researchers at a university.\n",
        "Could you suggest me a few catchy titles, free of jargon?\n",
        "\"\"\"\n",
        "\n",
        "response = local_llm(prompt, temperature=0.0)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rA8PW7E_KLP0",
        "outputId": "2cc56e11-d48b-4da2-9b91-4f6695db8cc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. \"Unlocking the Power of AI: Extracting Structured Information from Texts\"\n",
            "2. \"Transforming Text into Data: A Non-Technical Guide to AI Extraction\"\n",
            "3. \"Revolutionize Your Research: Harnessing AI to Extract Key Information\"\n",
            "4. \"From Text to Insights: A Workshop on AI-Driven Information Extraction\"\n",
            "5. \"AI for Everyone: Simplifying the Process of Extracting Structured Data\"\n",
            "6. \"Beyond Keywords: AI Techniques for Extracting Meaningful Information\"\n",
            "7. \"AI-Assisted Research: Extracting Structured Data from Unstructured Text\"\n",
            "8. \"AI Meets Research: A Hands-On Workshop on Extracting Structured Information\"\n",
            "9. \"AI-Powered Text Analysis: Extracting Valuable Data for Your Research\"\n",
            "10. \"AI and You: A Beginner's Guide to Extracting Structured Information from Texts\"\n"
          ]
        }
      ],
      "source": [
        "response = local_llm(prompt, temperature=0.0)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-lQJeUoHie3",
        "outputId": "e418103d-9c35-4b6b-caa0-741a152f8abf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are some catchy titles for your workshop that avoid jargon:\n",
            "\n",
            "1. \"Unlocking the Secrets of Text: Extracting Valuable Information with AI\"\n",
            "2. \"Transforming Text into Data: Harnessing AI for Researchers\"\n",
            "3. \"Revolutionize Your Research: Extracting Structured Information with AI\"\n",
            "4. \"AI-Powered Text Analysis: Discover Hidden Insights in Your Research\"\n",
            "5. \"AI Tools for Researchers: Simplifying Text Analysis and Information Extraction\"\n",
            "6. \"Effortlessly Organize Your Research: Automating Information Extraction with AI\"\n",
            "7. \"Unlocking the Potential of Text Data: A Beginner's Guide to AI-Assisted Research\"\n",
            "8. \"Streamline Your Research Process: AI Techniques for Extracting Key Information\"\n",
            "9. \"From Text to Data: AI Solutions for Non-Technical Researchers\"\n",
            "10. \"Harness the Power of AI: Transforming Unstructured Data into Actionable Insights\"\n",
            "\n",
            "These titles emphasize the practical benefits and applications of using AI for text analysis and structured information extraction, making them suitable for a non-technical audience.\n"
          ]
        }
      ],
      "source": [
        "response = local_llm(prompt, temperature=0.9)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTQc29RmOmje",
        "outputId": "d9dc8b6c-4a29-4bf9-848e-c71581cd5176"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure! Here are some suggestions that are catchy and easy to understand:\n",
            "\n",
            "1. \"Unlocking Hidden Knowledge: Extracting Structured Information from Texts\"\n",
            "2. \"Transforming Texts into Actionable Data: A Workshop on LLM Techniques\"\n",
            "3. \"Beyond Text: Leveraging AI to Extract Valuable Insights\"\n",
            "4. \"Revolutionize Your Research: Using Language Models to Extract Data\"\n",
            "5. \"From Text to Tables: Harnessing AI Technologies to Organize Information\"\n",
            "6. \"Structuring Knowledge: A Workshop on Extracting Meaningful Data from Text\"\n",
            "7. \"AI-Powered Text Analysis: Extracting Structured Information for Researchers\"\n",
            "8. \"Transforming Texts into Actionable Insights with AI Technologies\"\n",
            "9. \"From Unstructured to Structured Data: A Workshop on Using Language Models\"\n",
            "10. \"Boost Your Research with AI: Extracting Structured Information from Text\"\n",
            "\n",
            "Feel free to use any of these or modify them according to your needs. I hope these suggestions help!\n"
          ]
        }
      ],
      "source": [
        "response = local_llm(prompt, temperature=0.9)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUorb-qjzVcw"
      },
      "source": [
        "## Number of input / output tokens\n",
        "\n",
        "- What is a token?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1k46xd5OtyW",
        "outputId": "7f91736b-8660-4bbf-fff5-df765f681dfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Unlocking the Power of AI: Extracting Key Information from Texts\"\n",
            "\"H\n"
          ]
        }
      ],
      "source": [
        "response = local_llm(prompt, max_tokens=20)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.gutenberg.org/cache/epub/100/pg100.txt -P $working_dir\n",
        "long_text = (working_dir / \"pg100.txt\").read_text(encoding=\"utf-8\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mDVKIGo9k7K",
        "outputId": "7ebed8cd-5320-41fe-b12b-b5cb8c214345"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-03 09:05:17--  https://www.gutenberg.org/cache/epub/100/pg100.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5638519 (5.4M) [text/plain]\n",
            "Saving to: ‘/nvme/storage_michiel/llm_workshop/pg100.txt.1’\n",
            "\n",
            "pg100.txt.1         100%[===================>]   5.38M  6.13MB/s    in 0.9s    \n",
            "\n",
            "2024-04-03 09:05:18 (6.13 MB/s) - ‘/nvme/storage_michiel/llm_workshop/pg100.txt.1’ saved [5638519/5638519]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(long_text[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5lokUP-_kVM",
        "outputId": "3a501927-67c4-4d15-dc09-50926ab34008"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿The Project Gutenberg eBook of The Complete Works of William Shakespeare\n",
            "    \n",
            "This ebook is for the use of anyone anywhere in the United States and\n",
            "most other parts of the world at no cost and with almost no restrictions\n",
            "whatsoever. You may copy it, give it away or re-use it under the terms\n",
            "of the Project Gutenberg License included with this ebook or online\n",
            "at www.gutenberg.org. If you are not located in the United States,\n",
            "you will have to check the laws of the country where you are located\n",
            "bef\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "long_prompt = \"Please summarize the following: \\n\" + long_text\n",
        "response = local_llm(long_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cmbBXwOA_WEw",
        "outputId": "9e4b02d7-55f6-4f3d-88b4-ecaab75253ea"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Requested tokens (1748075) exceed context window of 8192",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[105], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m long_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease summarize the following: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m long_text\n\u001b[0;32m----> 2\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mlocal_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlong_prompt\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[94], line 16\u001b[0m, in \u001b[0;36mlocal_llm\u001b[0;34m(prompt, verbose, apply_template, temperature, max_tokens)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[0;32m/nvme/storage_michiel/miniforge3/envs/cu118/lib/python3.10/site-packages/llama_cpp/llama.py:1541\u001b[0m, in \u001b[0;36mLlama.__call__\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m   1478\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1479\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1503\u001b[0m     logit_bias: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1504\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[CreateCompletionResponse, Iterator[CreateCompletionStreamResponse]]:\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate text from a prompt.\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m \n\u001b[1;32m   1507\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;124;03m        Response object containing the generated text.\u001b[39;00m\n\u001b[1;32m   1540\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mecho\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mecho\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1561\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1566\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1567\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/nvme/storage_michiel/miniforge3/envs/cu118/lib/python3.10/site-packages/llama_cpp/llama.py:1474\u001b[0m, in \u001b[0;36mLlama.create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1472\u001b[0m     chunks: Iterator[CreateCompletionStreamResponse] \u001b[38;5;241m=\u001b[39m completion_or_chunks\n\u001b[1;32m   1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n\u001b[0;32m-> 1474\u001b[0m completion: Completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcompletion_or_chunks\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m completion\n",
            "File \u001b[0;32m/nvme/storage_michiel/miniforge3/envs/cu118/lib/python3.10/site-packages/llama_cpp/llama.py:953\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ctx\u001b[38;5;241m.\u001b[39mreset_timings()\n\u001b[1;32m    952\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(prompt_tokens) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_ctx:\n\u001b[0;32m--> 953\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    954\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested tokens (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(prompt_tokens)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exceed context window of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mllama_cpp\u001b[38;5;241m.\u001b[39mllama_n_ctx(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    955\u001b[0m     )\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m max_tokens \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;66;03m# Unlimited, depending on n_ctx.\u001b[39;00m\n\u001b[1;32m    959\u001b[0m     max_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_ctx \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(prompt_tokens)\n",
            "\u001b[0;31mValueError\u001b[0m: Requested tokens (1748075) exceed context window of 8192"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a34gj5t1XjBX"
      },
      "source": [
        "# Prompt Engineering 101\n",
        "\n",
        "- Zero shot learning\n",
        "- Few shot learning\n",
        "- Chain of thought\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "MPQ5XuPsWzAU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d67df28f-a096-4c08-dd50-062b49db1f39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The text can be classified as neutral.\n"
          ]
        }
      ],
      "source": [
        "# @title Zero-shot prompting\n",
        "prompt = \"\"\"\n",
        "Classify the text into neutral, negative or positive.\n",
        "Text: I think the workshop is okay.\n",
        "\"\"\"\n",
        "print(local_llm(prompt))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Few-shot prompting\n",
        "prompt = \"\"\"\n",
        "A \"whatpu\" is a small, furry animal native to Tanzania. An example of a sentence that uses the word whatpu is:\n",
        "We were traveling in Africa and we saw these very cute whatpus.\n",
        "\n",
        "To do a \"farduddle\" means to jump up and down really fast. Please give an example of a sentence that uses the word farduddle.\n",
        "\"\"\"\n",
        "local_llm(prompt)"
      ],
      "metadata": {
        "id": "whPnljsSQjxd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8778135-cbb7-43c7-ee70-762eac2d744e"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I was so excited about the surprise party that I couldn't help but do a little farduddle when I found out about it.\""
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Chain-of-thought prompting\n",
        "\n",
        "prompt_no_cot_formatted = \"\"\"\n",
        "<|im_start|>user\n",
        "Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls.\n",
        "How many tennis balls does he have now?\n",
        "<|im_end|>\n",
        "<|im_start|>assistant\n",
        "The answer is 11.\n",
        "<|im_end|>\n",
        "<|im_start|>user\n",
        "The cafetaria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
        "<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "print(local_llm(prompt_no_cot_formatted, apply_template=False, verbose=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJ72m_2pDA4S",
        "outputId": "046bf28b-ad3c-40e5-ed1e-6acf5f665edc"
      },
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt:\n",
            "\n",
            "<|im_start|>user\n",
            "Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls.\n",
            "How many tennis balls does he have now?\n",
            "<|im_end|>\n",
            "<|im_start|>assistant\n",
            "The answer is 11.\n",
            "<|im_end|>\n",
            "<|im_start|>user\n",
            "The cafetaria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
            "<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "After using 20 apples, they had 23 - 20 = 3 apples left. Then, they bought 6 more apples, so they had a total of 3 + 6 = 9 apples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_cot_formatted =  \"\"\"\n",
        "<|im_start|>user\n",
        "Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls.\n",
        "How many tennis balls does he have now?\n",
        "<|im_end|>\n",
        "<|im_start|>assistant\n",
        "Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
        "<|im_end|>\n",
        "<|im_start|>user\n",
        "The cafetaria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
        "<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "print(local_llm(prompt_cot_formatted, apply_template=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOliGtB7EjVD",
        "outputId": "b08b3746-082e-4854-aa88-0ff0834e4ef4"
      },
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cafetaria started with 23 apples. They used 20 for lunch, which left them with 23 - 20 = 3 apples. Then they bought 6 more apples, so they had a total of 3 + 6 = 9 apples. The answer is 9.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Zero-shot chain-of-thought\n",
        "\n",
        "prompt_cot_zs = \"\"\"\n",
        "<|im_start|>user\n",
        "The cafetaria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
        "<|im_end|>\n",
        "<|im_start|>assistant\n",
        "Let's think step by step: \"\"\"\n",
        "print(local_llm(prompt_cot_zs, apply_template=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mDjr6EeTwiQ",
        "outputId": "6cf58187-2e29-40a3-8592-a7b8444c1efd"
      },
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. The cafeteria initially has 23 apples.\n",
            "2. They use 20 apples to make lunch. So, we need to subtract these from the initial amount: 23 - 20 = 3.\n",
            "3. Then, they buy 6 more apples. We need to add these to the remaining amount: 3 + 6 = 9.\n",
            "4. Therefore, the cafeteria now has 9 apples.\n",
            "\n",
            "The final answer is that the cafeteria has 9 apples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Scaling up\n",
        "\n",
        "- Prompt template\n",
        "- Structure output\n",
        "- Retry until structure is valid\n",
        "- External APIs\n"
      ],
      "metadata": {
        "id": "8xpIgOIbcT3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Fetch Data and Load Into Pandas\n",
        "%%capture\n",
        "\n",
        "!wget \"http://datascience.web.rug.nl/llm_parliamentary_sample.csv\" -P $working_dir\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv(working_dir / \"llm_parliamentary_sample.csv\")"
      ],
      "metadata": {
        "id": "cRs-LOUwcVZa",
        "cellView": "form"
      },
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.query(\"votes_diff > 0\").head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "hK5O6ZujcsME",
        "outputId": "f1a00877-9f5a-486f-9c05-4b7051528636"
      },
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  url  \\\n",
              "8   https://www.theyworkforyou.com/wrans/?id=2022-...   \n",
              "13  https://www.theyworkforyou.com/wrans/?id=2022-...   \n",
              "20  https://www.theyworkforyou.com/wrans/?id=2022-...   \n",
              "29  https://www.theyworkforyou.com/wrans/?id=2022-...   \n",
              "35  https://www.theyworkforyou.com/wrans/?id=2022-...   \n",
              "\n",
              "                                     title  \\\n",
              "8               Detention Centres: Manston   \n",
              "13              Aircraft: Air Conditioning   \n",
              "20           Hypothyroidism: Prescriptions   \n",
              "29  Short-term Holding Facilities: Manston   \n",
              "35  Undocumented Migrants: English Channel   \n",
              "\n",
              "                              department date_submitted date_answered  \\\n",
              "8                            Home Office     2022-11-29    2023-01-03   \n",
              "13              Department for Transport     2022-12-16    2023-01-03   \n",
              "20  Department of Health and Social Care     2022-12-15    2023-01-03   \n",
              "29                           Home Office     2022-12-15    2023-01-03   \n",
              "35                           Home Office     2022-12-15    2023-01-03   \n",
              "\n",
              "                    question_speaker  \\\n",
              "8                    Stephen Kinnock   \n",
              "13  Baroness Bennett of Manor Castle   \n",
              "20          Lord Hunt of Kings Heath   \n",
              "29                       Lord Rosser   \n",
              "35                       Lord Rosser   \n",
              "\n",
              "                                    question_position  \\\n",
              "8         Shadow Minister (Home Office) (Immigration)   \n",
              "13                                              Green   \n",
              "20                                             Labour   \n",
              "29  Shadow Spokesperson (Home Affairs), Shadow Spo...   \n",
              "35  Shadow Spokesperson (Home Affairs), Shadow Spo...   \n",
              "\n",
              "                                        question_text  \\\n",
              "8   To ask the Secretary of State for the Home Dep...   \n",
              "13  To ask His Majesty's Government, with regards ...   \n",
              "20  To ask His Majesty's Government what discussio...   \n",
              "29  To ask His Majesty's Government what has been ...   \n",
              "35  To ask His Majesty's Government, further to th...   \n",
              "\n",
              "               answer_speaker  \\\n",
              "8              Robert Jenrick   \n",
              "13  Baroness Vere of Norbiton   \n",
              "20               Lord Markham   \n",
              "29   Lord Murray of Blidworth   \n",
              "35   Lord Murray of Blidworth   \n",
              "\n",
              "                                      answer_position  \\\n",
              "8                        The Minister for Immigration   \n",
              "13  Parliamentary Under-Secretary (Department for ...   \n",
              "20  The Parliamentary Under-Secretary for Health a...   \n",
              "29  The Parliamentary Under-Secretary of State for...   \n",
              "35  The Parliamentary Under-Secretary of State for...   \n",
              "\n",
              "                                          answer_text  votes_answered  \\\n",
              "8   .The HMIP report lists 6 Priority Concerns and...               1   \n",
              "13  The UK is rightly proud of its excellent recor...               1   \n",
              "20  There are no current plans to have discussions...               1   \n",
              "29  The costs of advice cannot be accurately calcu...               0   \n",
              "35  There are no plans to publish further details ...               0   \n",
              "\n",
              "    votes_notanswered  votes_diff  attachment  \n",
              "8                   2           1         NaN  \n",
              "13                  2           1         NaN  \n",
              "20                  2           1         NaN  \n",
              "29                  1           1         NaN  \n",
              "35                  1           1         NaN  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url</th>\n",
              "      <th>title</th>\n",
              "      <th>department</th>\n",
              "      <th>date_submitted</th>\n",
              "      <th>date_answered</th>\n",
              "      <th>question_speaker</th>\n",
              "      <th>question_position</th>\n",
              "      <th>question_text</th>\n",
              "      <th>answer_speaker</th>\n",
              "      <th>answer_position</th>\n",
              "      <th>answer_text</th>\n",
              "      <th>votes_answered</th>\n",
              "      <th>votes_notanswered</th>\n",
              "      <th>votes_diff</th>\n",
              "      <th>attachment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>https://www.theyworkforyou.com/wrans/?id=2022-...</td>\n",
              "      <td>Detention Centres: Manston</td>\n",
              "      <td>Home Office</td>\n",
              "      <td>2022-11-29</td>\n",
              "      <td>2023-01-03</td>\n",
              "      <td>Stephen Kinnock</td>\n",
              "      <td>Shadow Minister (Home Office) (Immigration)</td>\n",
              "      <td>To ask the Secretary of State for the Home Dep...</td>\n",
              "      <td>Robert Jenrick</td>\n",
              "      <td>The Minister for Immigration</td>\n",
              "      <td>.The HMIP report lists 6 Priority Concerns and...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>https://www.theyworkforyou.com/wrans/?id=2022-...</td>\n",
              "      <td>Aircraft: Air Conditioning</td>\n",
              "      <td>Department for Transport</td>\n",
              "      <td>2022-12-16</td>\n",
              "      <td>2023-01-03</td>\n",
              "      <td>Baroness Bennett of Manor Castle</td>\n",
              "      <td>Green</td>\n",
              "      <td>To ask His Majesty's Government, with regards ...</td>\n",
              "      <td>Baroness Vere of Norbiton</td>\n",
              "      <td>Parliamentary Under-Secretary (Department for ...</td>\n",
              "      <td>The UK is rightly proud of its excellent recor...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>https://www.theyworkforyou.com/wrans/?id=2022-...</td>\n",
              "      <td>Hypothyroidism: Prescriptions</td>\n",
              "      <td>Department of Health and Social Care</td>\n",
              "      <td>2022-12-15</td>\n",
              "      <td>2023-01-03</td>\n",
              "      <td>Lord Hunt of Kings Heath</td>\n",
              "      <td>Labour</td>\n",
              "      <td>To ask His Majesty's Government what discussio...</td>\n",
              "      <td>Lord Markham</td>\n",
              "      <td>The Parliamentary Under-Secretary for Health a...</td>\n",
              "      <td>There are no current plans to have discussions...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>https://www.theyworkforyou.com/wrans/?id=2022-...</td>\n",
              "      <td>Short-term Holding Facilities: Manston</td>\n",
              "      <td>Home Office</td>\n",
              "      <td>2022-12-15</td>\n",
              "      <td>2023-01-03</td>\n",
              "      <td>Lord Rosser</td>\n",
              "      <td>Shadow Spokesperson (Home Affairs), Shadow Spo...</td>\n",
              "      <td>To ask His Majesty's Government what has been ...</td>\n",
              "      <td>Lord Murray of Blidworth</td>\n",
              "      <td>The Parliamentary Under-Secretary of State for...</td>\n",
              "      <td>The costs of advice cannot be accurately calcu...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>https://www.theyworkforyou.com/wrans/?id=2022-...</td>\n",
              "      <td>Undocumented Migrants: English Channel</td>\n",
              "      <td>Home Office</td>\n",
              "      <td>2022-12-15</td>\n",
              "      <td>2023-01-03</td>\n",
              "      <td>Lord Rosser</td>\n",
              "      <td>Shadow Spokesperson (Home Affairs), Shadow Spo...</td>\n",
              "      <td>To ask His Majesty's Government, further to th...</td>\n",
              "      <td>Lord Murray of Blidworth</td>\n",
              "      <td>The Parliamentary Under-Secretary of State for...</td>\n",
              "      <td>There are no plans to publish further details ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 240
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first_row = df.query(\"votes_diff > 0\").iloc[0]"
      ],
      "metadata": {
        "id": "siUK4s6ed4uT"
      },
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "metadata": {
        "id": "31eeyzdmxo0K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9998858-81bb-4e6c-fa16-10ec6e4ae177"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt:\n",
            "\n",
            "<|im_start|>user\n",
            "\n",
            "I will provide you a question and a response given in a parliamentary setting.\n",
            "\n",
            "The question:\n",
            "To ask the Secretary of State for the Home Department, what steps her Department took to act on the findings of the report by the Chief Inspector of Prisons into conditions at Manston asylum centre published in July 2022 which indicated that the facilities at Manston for managing people with infectious diseases were poor.\n",
            "\n",
            "The answer:\n",
            ".The HMIP report lists 6 Priority Concerns and 8 further Key Concerns which HMIP inspectors felt required addressing at Manston and Western Jetfoil. While one of the Priority Concerns (Priority Concern 3) referenced weaknesses in the governance of health care processes, no specific mention was made in any of the concerns about facilities at Manston for managing people with infectious diseases.The Home Office developed a Service Improvement Plan in response to the 14 Concerns listed in the report, and worked quickly with its medical contractors to ensure that the deficiencies highlighted in Priority Concern 3 were quickly addressed.\n",
            "\n",
            "Does the response sufficiently answer the question?\n",
            "\n",
            "Return your answer as a valid JSON object with a single field `final answer` with\n",
            "a boolean value with your final answer, like {\"final_answer\": …}.\n",
            "\n",
            "<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# @title Prompt templates\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "I will provide you a question and a response given in a parliamentary setting.\n",
        "\n",
        "The question:\n",
        "{question}\n",
        "\n",
        "The answer:\n",
        "{answer}\n",
        "\n",
        "Does the response sufficiently answer the question?\n",
        "\n",
        "Return your answer as a valid JSON object with a single field `final answer` with\n",
        "a boolean value with your final answer, like {{\"final_answer\": …}}.\n",
        "\"\"\"\n",
        "\n",
        "prompt = prompt_template.format(\n",
        "    question=first_row[\"question_text\"].strip(),\n",
        "    answer=first_row[\"answer_text\"].strip()\n",
        ")\n",
        "\n",
        "response = local_llm(prompt, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_prompt_template = \"\"\"\n",
        "<|im_start|>user\n",
        "I will provide you a question and a response given in a parliamentary setting.\n",
        "\n",
        "The question:\n",
        "{question}\n",
        "\n",
        "The answer:\n",
        "{answer}\n",
        "\n",
        "Does the response sufficiently answer the question?\n",
        "\n",
        "Return your answer as a valid JSON object with a single field `final answer` with\n",
        "a boolean value with your final answer, like {{\"final_answer\": …}}.\n",
        "<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "\n",
        "formatted_prompt = formatted_prompt_template.format(\n",
        "    question=first_row[\"question_text\"].strip(),\n",
        "    answer=first_row[\"answer_text\"].strip()\n",
        ")"
      ],
      "metadata": {
        "id": "wWBAMlIlLHKc"
      },
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = local_llm(formatted_prompt + \"Let's think step by step: \", apply_template=False, verbose=True)\n",
        "print(\"\\nLLM answer: \")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U43mhU-ALJ5j",
        "outputId": "e2faad4e-80bf-447c-b07d-66418fba743c"
      },
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt:\n",
            "\n",
            "<|im_start|>user\n",
            "I will provide you a question and a response given in a parliamentary setting.\n",
            "\n",
            "The question:\n",
            "To ask the Secretary of State for the Home Department, what steps her Department took to act on the findings of the report by the Chief Inspector of Prisons into conditions at Manston asylum centre published in July 2022 which indicated that the facilities at Manston for managing people with infectious diseases were poor.\n",
            "\n",
            "The answer:\n",
            ".The HMIP report lists 6 Priority Concerns and 8 further Key Concerns which HMIP inspectors felt required addressing at Manston and Western Jetfoil. While one of the Priority Concerns (Priority Concern 3) referenced weaknesses in the governance of health care processes, no specific mention was made in any of the concerns about facilities at Manston for managing people with infectious diseases.The Home Office developed a Service Improvement Plan in response to the 14 Concerns listed in the report, and worked quickly with its medical contractors to ensure that the deficiencies highlighted in Priority Concern 3 were quickly addressed.\n",
            "\n",
            "Does the response sufficiently answer the question?\n",
            "\n",
            "Return your answer as a valid JSON object with a single field `final answer` with\n",
            "a boolean value with your final answer, like {\"final_answer\": …}.\n",
            "<|im_end|>\n",
            "<|im_start|>assistant\n",
            "Let's think step by step: \n",
            "\n",
            "LLM answer: \n",
            "\n",
            "\n",
            "1. The Member of Parliament asked about the steps taken by the Home Department regarding the conditions at Manston asylum center and its facilities for managing people with infectious diseases as mentioned in the Chief Inspector's report published in July 2022.\n",
            "2. The minister's answer mentioned that there was no specific mention about facilities for managing infectious diseases in any of the concerns listed in the report.\n",
            "3. The minister said that they developed a Service Improvement Plan based on all the concerns listed in the report, and they focused on addressing the deficiencies related to Priority Concern 3 (weaknesses in governance of healthcare processes).\n",
            "\n",
            "Based on this analysis, the minister's answer does not directly address the specific concern about facilities for managing infectious diseases mentioned in the question. It only discusses the general response to the report's concerns. So, my final answer would be:\n",
            "\n",
            "{\n",
            "  \"final_answer\": false\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "json_expression = re.compile(r\"\\{.+?\\}\", re.DOTALL)"
      ],
      "metadata": {
        "id": "9phuM7jDYHfd"
      },
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answers = json_expression.findall(response)"
      ],
      "metadata": {
        "id": "8bffAk-aZ0jG"
      },
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "json.loads(answers[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeP4L1JkZ3kW",
        "outputId": "751342ca-b168-4f05-b69e-d86d16499e0f"
      },
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'final_answer': False}"
            ]
          },
          "metadata": {},
          "execution_count": 251
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FHJJbyYwaEQ5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}